{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCP Pipeline: SFTP >> GCS >> Big Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction on how to set up a pipeline triggered by a file \"PUT\" to an sftp server.\n",
    "Set up steps:\n",
    "1. SFTP server\n",
    "2. GCS bucket mirroring/sync on SFTP server\n",
    "3. Webhook trigger using cloud functions\n",
    "4. Airflow DAG (Composer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SFTP Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under Compute Engine, create a VM instace with the minimal compute and storage. This server will act as a pass-through to GCS.\n",
    "\n",
    "Since external sources will be connecting to this server, a static IP will need to be assigned. (Ephemeral by default)\n",
    "\n",
    "Specs I used for an hourly 200mb file dump:\n",
    "- machine type: f1-micro, 1vCPU, 0.6 GB memory\n",
    "- debian-9-stretch image\n",
    "- 10 GB SSD storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GCS Mirror/Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount GCS bucket onto SFTP server directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. GCS UI: Create a new bucket with unique name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SFTP Server: Install lftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install lftp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. SFTP Server: Install gcsfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export GCSFUSE_REPO=gcsfuse-`lsb_release -c -s`\n",
    "echo \"deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "\n",
    "sudo apt-get update\n",
    "sudo apt-get install gcsfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SFTP Server: Mount GCS Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace < replace-by-bucket-name > by bucket name (created in GCS UI). Do not use gs://"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ~/gcsBucket\n",
    "gcsfuse <replace-by-bucket-name> ~/gcsBucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test directory and GCS mirror:\n",
    "- create blank .txt file >> touch test.txt\n",
    "- move file to mounted dir >> mv /file_path/test.txt /directory_path/mounted_gcsBucket\n",
    "- (Ignore date warning while moving file)\n",
    "- open GCS UI and verify file has been moved over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cloud Functions - Webhook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Enable the Cloud Composer, Cloud Functions, and Cloud Identity and Access Management (Cloud IAM) APIs.\n",
    "\n",
    "Link: https://console.cloud.google.com/flows/enableapi?apiid=cloudfunctions,iam,composer&redirect=https://console.cloud.google.com&_ga=2.93559517.1591282240.1584371833-334489940.1583948449"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Grant blob signing permissions to the Cloud Functons Service Account:\n",
    "\n",
    "To authenticate to IAP, grant the Appspot Service Account (used by Cloud Functions) the Service Account Token Creator role on itself. To do this, execute the following command in the gcloud command-line tool or Cloud Shell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run in the cloud console:\n",
    "\n",
    "Insert/swap out below: your-project-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud iam service-accounts add-iam-policy-binding \\\n",
    "your-project-id@appspot.gserviceaccount.com \\\n",
    "--member=serviceAccount:your-project-id@appspot.gserviceaccount.com \\\n",
    "--role=roles/iam.serviceAccountTokenCreator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get the client ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a token to authenticate to IAP, the function requires the client ID of the proxy that protects the Airflow webserver. The Cloud Composer API does not provide this information directly. Instead, make an unauthenticated request to the Airflow webserver and capture the client ID from the redirect URL. The following Python code sample demonstrates how to get the client ID. After executing this code on the command line or in Cloud Shell, your client ID will be printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run python script in cloud console:\n",
    "\n",
    "Uncomment and update variables in script below--\n",
    "\n",
    "project_id = 'YOUR_PROJECT_ID'\n",
    "\n",
    "location = 'us-central1' # Choose your preferred region\n",
    "\n",
    "composer_environment = 'YOUR_COMPOSER_ENVIRONMENT_NAME'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "import requests\n",
    "import six.moves.urllib.parse\n",
    "\n",
    "# Authenticate with Google Cloud.\n",
    "# See: https://cloud.google.com/docs/authentication/getting-started\n",
    "credentials, _ = google.auth.default(\n",
    "    scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "authed_session = google.auth.transport.requests.AuthorizedSession(\n",
    "    credentials)\n",
    "\n",
    "# project_id = 'YOUR_PROJECT_ID'\n",
    "# location = 'us-central1'\n",
    "# composer_environment = 'YOUR_COMPOSER_ENVIRONMENT_NAME'\n",
    "\n",
    "environment_url = (\n",
    "    'https://composer.googleapis.com/v1beta1/projects/{}/locations/{}'\n",
    "    '/environments/{}').format(project_id, location, composer_environment)\n",
    "composer_response = authed_session.request('GET', environment_url)\n",
    "environment_data = composer_response.json()\n",
    "airflow_uri = environment_data['config']['airflowUri']\n",
    "\n",
    "# The Composer environment response does not include the IAP client ID.\n",
    "# Make a second, unauthenticated HTTP request to the web server to get the\n",
    "# redirect URI.\n",
    "redirect_response = requests.get(airflow_uri, allow_redirects=False)\n",
    "redirect_location = redirect_response.headers['location']\n",
    "\n",
    "# Extract the client_id query parameter from the redirect.\n",
    "parsed = six.moves.urllib.parse.urlparse(redirect_location)\n",
    "query_string = six.moves.urllib.parse.parse_qs(parsed.query)\n",
    "print(query_string['client_id'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIENT ID: Save/keep track of client ID for later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create cloud function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate over to the cloud functions UI in GCP: create a function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Name: your_dag_trigger_function\n",
    "- Memory Allocated: 128 MiB\n",
    "- Trigger: Cloud Storage\n",
    "- Event Type: Finalize/Create\n",
    "- Bucket: your-gcs-bucket-name\n",
    "- Source code: Inline editor\n",
    "- Runtime: Python 3.7\n",
    "- Function to execute: trigger_your_dag\n",
    "\n",
    "\n",
    "### main.py & requirements.txt will need to be edited:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_toolbelt==0.9.1\n",
    "google-auth==1.11.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.py: 3 edits required in line 25 (script can be copied and pasted edits are made)\n",
    "- client_id: 'This was printed in the gconsole in step 3'\n",
    "- webserver_id: 'This can be found in the URL address bar when in the Airflow UI (composer)-- https:// < your_web_server_id > .appspot.com/'\n",
    "- dag_name: '< your_dag_name >'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import google.auth.compute_engine.credentials\n",
    "import google.auth.iam\n",
    "from google.auth.transport.requests import Request\n",
    "import google.oauth2.credentials\n",
    "import google.oauth2.service_account\n",
    "import requests\n",
    "\n",
    "IAM_SCOPE = 'https://www.googleapis.com/auth/iam'\n",
    "OAUTH_TOKEN_URI = 'https://www.googleapis.com/oauth2/v4/token'\n",
    "\n",
    "def trigger_dag(data, context=None):\n",
    "    \"\"\"Makes a POST request to the Composer DAG Trigger API\n",
    "\n",
    "    When called via Google Cloud Functions (GCF),\n",
    "    data and context are Background function parameters.\n",
    "\n",
    "    For more info, refer to\n",
    "    https://cloud.google.com/functions/docs/writing/background#functions_background_parameters-python\n",
    "\n",
    "    To call this function from a Python script, omit the ``context`` argument\n",
    "    and pass in a non-null value for the ``data`` argument.\n",
    "    \"\"\"\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # Fill in with your Composer info here\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    \n",
    "    client_id = 'YOUR-CLIENT-ID'\n",
    "    \n",
    "    # This should be part of your webserver's URL:\n",
    "    # {tenant-project-id}.appspot.com\n",
    "    webserver_id = 'YOUR-TENANT-PROJECT'\n",
    "    \n",
    "    # The name of the DAG you wish to trigger\n",
    "    dag_name = 'composer_sample_trigger_response_dag'\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # End of info fill in\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    \n",
    "    webserver_url = (\n",
    "        'https://'\n",
    "        + webserver_id\n",
    "        + '.appspot.com/api/experimental/dags/'\n",
    "        + dag_name\n",
    "        + '/dag_runs'\n",
    "    )\n",
    "    # Make a POST request to IAP which then Triggers the DAG\n",
    "    make_iap_request(webserver_url, client_id, method='POST', json={\"conf\":data})\n",
    "\n",
    "\n",
    "# This code is copied from\n",
    "# https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/iap/make_iap_request.py\n",
    "# START COPIED IAP CODE\n",
    "def make_iap_request(url, client_id, method='GET', **kwargs):\n",
    "    \"\"\"Makes a request to an application protected by Identity-Aware Proxy.\n",
    "\n",
    "    Args:\n",
    "      url: The Identity-Aware Proxy-protected URL to fetch.\n",
    "      client_id: The client ID used by Identity-Aware Proxy.\n",
    "      method: The request method to use\n",
    "              ('GET', 'OPTIONS', 'HEAD', 'POST', 'PUT', 'PATCH', 'DELETE')\n",
    "      **kwargs: Any of the parameters defined for the request function:\n",
    "                https://github.com/requests/requests/blob/master/requests/api.py\n",
    "                If no timeout is provided, it is set to 90 by default.\n",
    "\n",
    "    Returns:\n",
    "      The page body, or raises an exception if the page couldn't be retrieved.\n",
    "    \"\"\"\n",
    "    # Set the default timeout, if missing\n",
    "    if 'timeout' not in kwargs:\n",
    "        kwargs['timeout'] = 90\n",
    "\n",
    "    # Figure out what environment we're running in and get some preliminary\n",
    "    # information about the service account.\n",
    "    bootstrap_credentials, _ = google.auth.default(\n",
    "        scopes=[IAM_SCOPE])\n",
    "\n",
    "    # For service account's using the Compute Engine metadata service,\n",
    "    # service_account_email isn't available until refresh is called.\n",
    "    bootstrap_credentials.refresh(Request())\n",
    "\n",
    "    signer_email = bootstrap_credentials.service_account_email\n",
    "    if isinstance(bootstrap_credentials,\n",
    "                  google.auth.compute_engine.credentials.Credentials):\n",
    "        # Since the Compute Engine metadata service doesn't expose the service\n",
    "        # account key, we use the IAM signBlob API to sign instead.\n",
    "        # In order for this to work:\n",
    "        # 1. Your VM needs the https://www.googleapis.com/auth/iam scope.\n",
    "        #    You can specify this specific scope when creating a VM\n",
    "        #    through the API or gcloud. When using Cloud Console,\n",
    "        #    you'll need to specify the \"full access to all Cloud APIs\"\n",
    "        #    scope. A VM's scopes can only be specified at creation time.\n",
    "        # 2. The VM's default service account needs the \"Service Account Actor\"\n",
    "        #    role. This can be found under the \"Project\" category in Cloud\n",
    "        #    Console, or roles/iam.serviceAccountActor in gcloud.\n",
    "        signer = google.auth.iam.Signer(\n",
    "            Request(), bootstrap_credentials, signer_email)\n",
    "    else:\n",
    "        # A Signer object can sign a JWT using the service account's key.\n",
    "        signer = bootstrap_credentials.signer\n",
    "\n",
    "    # Construct OAuth 2.0 service account credentials using the signer\n",
    "    # and email acquired from the bootstrap credentials.\n",
    "    service_account_credentials = google.oauth2.service_account.Credentials(\n",
    "        signer, signer_email, token_uri=OAUTH_TOKEN_URI, additional_claims={\n",
    "            'target_audience': client_id\n",
    "        })\n",
    "    # service_account_credentials gives us a JWT signed by the service\n",
    "    # account. Next, we use that to obtain an OpenID Connect token,\n",
    "    # which is a JWT signed by Google.\n",
    "    google_open_id_connect_token = get_google_open_id_connect_token(\n",
    "        service_account_credentials)\n",
    "\n",
    "    # Fetch the Identity-Aware Proxy-protected URL, including an\n",
    "    # Authorization header containing \"Bearer \" followed by a\n",
    "    # Google-issued OpenID Connect token for the service account.\n",
    "    resp = requests.request(\n",
    "        method, url,\n",
    "        headers={'Authorization': 'Bearer {}'.format(\n",
    "            google_open_id_connect_token)}, **kwargs)\n",
    "    if resp.status_code == 403:\n",
    "        raise Exception('Service account {} does not have permission to '\n",
    "                        'access the IAP-protected application.'.format(\n",
    "                            signer_email))\n",
    "    elif resp.status_code != 200:\n",
    "        raise Exception(\n",
    "            'Bad response from application: {!r} / {!r} / {!r}'.format(\n",
    "                resp.status_code, resp.headers, resp.text))\n",
    "    else:\n",
    "        return resp.text\n",
    "\n",
    "\n",
    "def get_google_open_id_connect_token(service_account_credentials):\n",
    "    \"\"\"Get an OpenID Connect token issued by Google for the service account.\n",
    "\n",
    "    This function:\n",
    "\n",
    "      1. Generates a JWT signed with the service account's private key\n",
    "         containing a special \"target_audience\" claim.\n",
    "\n",
    "      2. Sends it to the OAUTH_TOKEN_URI endpoint. Because the JWT in #1\n",
    "         has a target_audience claim, that endpoint will respond with\n",
    "         an OpenID Connect token for the service account -- in other words,\n",
    "         a JWT signed by *Google*. The aud claim in this JWT will be\n",
    "         set to the value from the target_audience claim in #1.\n",
    "\n",
    "    For more information, see\n",
    "    https://developers.google.com/identity/protocols/OAuth2ServiceAccount .\n",
    "    The HTTP/REST example on that page describes the JWT structure and\n",
    "    demonstrates how to call the token endpoint. (The example on that page\n",
    "    shows how to get an OAuth2 access token; this code is using a\n",
    "    modified version of it to get an OpenID Connect token.)\n",
    "    \"\"\"\n",
    "\n",
    "    service_account_jwt = (\n",
    "        service_account_credentials._make_authorization_grant_assertion())\n",
    "    request = google.auth.transport.requests.Request()\n",
    "    body = {\n",
    "        'assertion': service_account_jwt,\n",
    "        'grant_type': google.oauth2._client._JWT_GRANT_TYPE,\n",
    "    }\n",
    "    token_response = google.oauth2._client._token_endpoint_request(\n",
    "        request, OAUTH_TOKEN_URI, body)\n",
    "    return token_response['id_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Airflow DAG (Composer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import airflow\n",
    "from airflow import DAG\n",
    "from airflow import models\n",
    "from airflow import configuration\n",
    "from airflow.models import Variable\n",
    "from airflow.models import TaskInstance\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "from airflow.contrib.hooks import gcs_hook\n",
    "from airflow.operators import bash_operator\n",
    "from airflow.contrib.operators import gcs_to_bq, gcs_to_gcs\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\"\"\"\n",
    "GCS trigger (cloudfunction) >> GCS file to Big Query\n",
    "Files dropped into gcs bucket will trigger this DAG and sink to big query table\n",
    "\"\"\"\n",
    "default_args={\n",
    "    'owner': 'your_name_here',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime.datetime(1999, 5, 17, 0, 0)\n",
    "    'email': [''],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'end_date': datetime(2016, 1, 1),\n",
    "    # 'wait_for_downstream': False,\n",
    "    # 'dag': dag,\n",
    "    # 'sla': timedelta(hours=2),\n",
    "    # 'execution_timeout': timedelta(seconds=300),\n",
    "    # 'on_failure_callback': some_function,\n",
    "    # 'on_success_callback': some_other_function,\n",
    "    # 'on_retry_callback': another_function,\n",
    "    # 'sla_miss_callback': yet_another_function,\n",
    "    # 'trigger_rule': 'all_success'\n",
    "}\n",
    "\n",
    "# Input Params\n",
    "compression_type='GZIP', # or default value 'NONE' \n",
    "field_delimiter_type=\"\\t\", # tab delimited or \",\" for comma delimited\n",
    "source_bucket='your_source_GCSbucket'\n",
    "destination_dataset='your_bq_data_set'\n",
    "destination_table='your_bq_table'\n",
    "write_disposition_type='WRITE_APPEND' \n",
    "bq_schema=[\n",
    "    {'name': 'username', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "    {'name': 'date_time', 'type': 'TIMESTAMP', 'mode': 'NULLABLE'},\n",
    "    {'name': 'ip', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "]\n",
    "\n",
    "def get_source(**kwargs):\n",
    "    \"\"\"\n",
    "    gets file_name from DAG context and xcom_pushes\n",
    "    \"\"\"\n",
    "    split = \"\\/(.*?)\\/\"\n",
    "    context_id = kwargs[\"dag_run\"].conf['id']\n",
    "    gcs_file_name = re.search(split, context_id).group(1)\n",
    "    task_instance = kwargs['task_instance']\n",
    "    task_instance.xcom_push(value=gcs_file_name, key='file_name')\n",
    "    \n",
    "def branch(**kwargs):\n",
    "    task_instance = kwargs['task_instance']\n",
    "    file_name = task_instance.xcom_pull(task_ids='get_file', key='file_name')\n",
    "    print(\"FILE NAME: \"+file_name)\n",
    "    if file_name.endswith('.tsv.gz'):\n",
    "        return 'sink_bq'\n",
    "    else:\n",
    "        return 'move_meta_file'\n",
    "    \n",
    "with airflow.DAG(\n",
    "    'sftp_to_bigquery',\n",
    "    default_args=default_args,\n",
    "    description='GCS trigger to bigquery',\n",
    "    max_active_runs=6, # max number of concurrent dag runs (best practice n_cores-1)\n",
    "    schedule_interval=None) as dag:\n",
    "    \n",
    "    \n",
    "    # Runs get_source function to save file name to xcom\n",
    "    get_file = PythonOperator(task_id=\"get_file\", python_callable=get_source, provide_context=True)\n",
    "\n",
    "    fork = BranchPythonOperator(task_id='fork', python_callable=branch, provide_context=True)\n",
    "\n",
    "    sink_bq = gcs_to_bq.GoogleCloudStorageToBigQueryOperator(\n",
    "        task_id='sink_bq',\n",
    "        bucket=source_bucket,\n",
    "        source_objects=[\"{{ task_instance.xcom_pull(task_ids='get_file', key='file_name') }}\"], # pulls file_name\n",
    "        destination_project_dataset_table='your_dataset.your_table',\n",
    "        schema_fields=bq_schema,\n",
    "        compression=compression_type,\n",
    "        field_delimiter=field_delimiter_type,\n",
    "        write_disposition=write_disposition_type,\n",
    "        provide_context=True\n",
    "        )\n",
    "\n",
    "    move_sink_file = gcs_to_gcs.GoogleCloudStorageToGoogleCloudStorageOperator(\n",
    "        task_id='move_sink_file',\n",
    "        source_bucket=source_bucket,\n",
    "        source_object=\"{{ task_instance.xcom_pull(task_ids='get_file', key='file_name') }}\",\n",
    "        destination_bucket=del_bucket,\n",
    "        destination_object=\"{{ task_instance.xcom_pull(task_ids='get_file', key='file_name') }}\",\n",
    "        move_object=True,\n",
    "        provide_context=True,\n",
    "        trigger_rule='none_failed'\n",
    "        )\n",
    "\n",
    "    move_meta_file = gcs_to_gcs.GoogleCloudStorageToGoogleCloudStorageOperator(\n",
    "        task_id='move_meta_file',\n",
    "        source_bucket=source_bucket,\n",
    "        source_object=\"{{ task_instance.xcom_pull(task_ids='get_file', key='file_name') }}\",\n",
    "        destination_bucket=del_bucket,\n",
    "        destination_object=\"{{ task_instance.xcom_pull(task_ids='get_file', key='file_name') }}\",\n",
    "        move_object=True,\n",
    "        provide_context=True,\n",
    "        trigger_rule='none_failed'\n",
    "        )\n",
    "\n",
    "    job_complete = DummyOperator(task_id='job_complete', dag=dag, trigger_rule='none_failed')\n",
    "\n",
    "    get_file >> fork \n",
    "    fork >> move_meta_file >> job_complete\n",
    "    fork >> sink_bq >> move_sink_file >> job_complete\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
